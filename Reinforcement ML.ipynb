{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "---\n",
    "Reinforement learning is about taking a suitable action to maximize reward in a particular situation. In reinforcement learning, agents are self-trained on reward and punishment mechanisms. The idea is to take the path to maximum rewards and least punishments. Through trial and error, an agent keeps learning continously in an interactive environment from its own actions and experiences. It learns through feedback and interaction.\n",
    "\n",
    "According to Gartner, reinforcement learning is based on rewarding desired behaviors or puninshing underired ones. Instead of one input producing one output, the algorithm produces a variety of outputs and is trained to select the right one based on certain variables.\n",
    "\n",
    "Reinforcement learning differs from supervised machine learning in that it has no label and gets to decide how to perform a given task. This could be either with a trained dataset or the absense of one. \n",
    "\n",
    "Terminologies of Reinforcement Learning: <br>\n",
    "* **Agent** - Decision maker or learner.\n",
    "* **Environment** - Physical world where an agent learns and decides on the action to take.\n",
    "* **Action** - List of events that an agent can perform.\n",
    "* **State** - Current situation of the agent in the environment.\n",
    "* **Reward** - This is what the environment gives for every action by agent. Usually, it is a scalar value and nothing but feedback from the environment.\n",
    "* **Policy** - Strategies that are prepared by the agent to map situations to actions.\n",
    "* **Value Function** - Value of state shows up the reward achieved starting from the state until the policy is executed.\n",
    "* **Model** - \n",
    "\n",
    "Types of Reinforcement Learning:\n",
    "1. Positive Reinforcement\n",
    "When an event occurs due to a specific behavior, it increases a positive impact on the strength and frequency of the behavior.\n",
    "\n",
    "2. Negative Reinforcement\n",
    "This is the strenghtening of a behavior when a negative condition is stopped, barred and avoided in the future.\n",
    "\n",
    "Flow of Reinforcement Learning:\n",
    "1. Create the environment\n",
    "2. Define the reward\n",
    "3. Create the agent\n",
    "4. Train and validate the agent (rewards and punishments)\n",
    "5. Deploy the policy\n",
    "\n",
    "Reinforcement Learning Algorithms:\n",
    "- **Value Based:** Main goal is to maximize the value function. The agent through a policy expects a long-term return of the current states.\n",
    "- **Policy Based (Deterministic and Stochastic):** In this startegy, you enable to come up with a strategy that helps to gain maximum rewards in the future through possible actions performed in each state.\n",
    "- **Model-Based:** We need to create a virtual model for the agent to help in learning to perform in each specific environment.\n",
    "\n",
    "Models for Reinforcement Learning:\n",
    "- Markov Decision Process\n",
    "- Q Learning (Value-Based)\n",
    "\n",
    "Application of Reinforcement learning:\n",
    "- Robotics for industrial automation\n",
    "- Text summarization engines\n",
    "- Autonomous self driving cars\n",
    "- Aircraft control and robot motion control\n",
    "\n",
    "Sources: <br>\n",
    "https://www.geeksforgeeks.org/what-is-reinforcement-learning/ <br>\n",
    "https://www.analyticsvidhya.com/blog/2021/02/introduction-to-reinforcement-learning-for-beginners/ <br>\n",
    "https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pygame in /home/wambui/.local/lib/python3.8/site-packages (2.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wambui/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  class IteratorBase(collections.Iterator, trackable.Trackable,\n",
      "/home/wambui/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:106: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  class DatasetV2(collections.Iterable, tracking_base.Trackable,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import rl\n",
    "\n",
    "# from rl.agents.dqn import DQNAgent\n",
    "# from rl.policy import EpsGreedyQPolicy\n",
    "# from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wambui/.local/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/wambui/.local/lib/python3.8/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'rl' has no attribute 'policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-efd6e54df8c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpsGreedyQPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n\u001b[1;32m      4\u001b[0m target_model_update=1e-2, policy=policy)\n\u001b[1;32m      5\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'rl' has no attribute 'policy'"
     ]
    }
   ],
   "source": [
    "policy = rl.policy.EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
